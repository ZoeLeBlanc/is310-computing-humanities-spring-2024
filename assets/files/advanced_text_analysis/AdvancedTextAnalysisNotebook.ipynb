{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Text Analysis Notebook\n",
    "\n",
    "This is a notebook to help you experiment with text analysis methods. The goal is to try your hand at understanding how these transformations happen and how they can help us understand patterns in unstructured text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, try commenting each of the libraries with a small note about what they do and a link to their documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both datasets are available in Google Drive at this link [https://drive.google.com/drive/folders/1zNTPmQm_HtR1zeCpRAp3XSDBsZuZq0U5?usp=sharing](https://drive.google.com/drive/folders/1zNTPmQm_HtR1zeCpRAp3XSDBsZuZq0U5?usp=sharing). Download the datasets and load them into the notebook. See if you can understand the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Humanist Listserv dataset\n",
    "humanist_vols = pd.read_csv(\"web_scraped_humanist_listserv_volumes.csv\")\n",
    "pudding_data = pd.read_csv(\"categorized_pudding_public_scripts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code from our lesson in class. Try running it and see if you can understand what it does. Then, try to modify it to see if you can get different results. Use the documentation to help you understand what each parameter does [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). In particular, try out the `max_df`, `min_df`, and `ngram_range` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save our texts to a list\n",
    "documents = humanist_vols.volume_text.tolist()\n",
    "\n",
    "#Create a vectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=.8)\n",
    "# Fit the vectorizer to our documents\n",
    "transformed_documents = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Now get the top features for each document\n",
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "\n",
    "# Get the dates for each volume\n",
    "dates = humanist_vols.inferred_start_year.tolist()\n",
    "\n",
    "# Create an empty list to store our results\n",
    "tfidf_results = []\n",
    "\n",
    "# Loop through each document and get the top terms\n",
    "for counter, doc in enumerate(transformed_documents_as_array):\n",
    "    # Zip together the terms and the scores\n",
    "    tf_idf_tuples = list(zip(vectorizer.get_feature_names_out(), doc))\n",
    "    # Sort the terms by score\n",
    "    one_doc_as_df = pd.DataFrame.from_records(tf_idf_tuples, columns=['term', 'score']).sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "    # Add the date to the dataframe\n",
    "    one_doc_as_df['inferred_start_year'] = dates[counter]\n",
    "    # Append the dataframe to our list\n",
    "    tfidf_results.append(one_doc_as_df)\n",
    "# Concatenate all the dataframes together\n",
    "tfidf_df = pd.concat(tfidf_results)\n",
    "# Sort the dataframe by score\n",
    "tfidf_df = tfidf_df.sort_values(by=['score'], ascending=False)\n",
    "# Get the top ten terms for each year\n",
    "top_terms = tfidf_df.groupby('inferred_start_year').apply(lambda x: x.sort_values('score', ascending=False).head(10)).reset_index(drop=True)\n",
    "# Convert the inferred_start_year to a datetime\n",
    "top_terms['inferred_start_year'] = pd.to_datetime(top_terms['inferred_start_year'], format='%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = alt.selection_point(fields=['term'], bind='legend')\n",
    "chart = alt.Chart(top_terms).mark_bar().encode(\n",
    "    y='score',\n",
    "    x='inferred_start_year:T',\n",
    "    color=alt.Color('term', legend=alt.Legend(title='Term', orient='right', symbolLimit=len(top_terms['term'].unique()), columns=5), scale=alt.Scale(scheme='tableau20')),\n",
    "    tooltip=['term', 'score', 'year(inferred_start_year)'],\n",
    "    opacity=alt.condition(selection, alt.value(1), alt.value(0.2))\n",
    ").add_params(selection).properties(\n",
    "    title='Top 10 Terms by TF-IDF Score in Humanist Volumes'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Periodization of Humanist Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the volumes by period\n",
    "humanist_vols['period'] = pd.cut(humanist_vols['inferred_start_year'], bins=[float('-inf'), 2000, 2010, 2020], labels=['early_internet', 'web_2.0', 'contemporary'])\n",
    "# Create a vectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=.8)\n",
    "# Fit the vectorizer to our documents\n",
    "transformed_documents = vectorizer.fit_transform(humanist_vols.groupby('period')['volume_text'].apply(' '.join).tolist())\n",
    "# Now get the top features for each document\n",
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "# Get the periods for each volume\n",
    "periods = humanist_vols['period'].unique()\n",
    "# Create an empty list to store our results\n",
    "tfidf_results = []\n",
    "# Loop through each document and get the top terms\n",
    "for counter, doc in enumerate(transformed_documents_as_array):\n",
    "    # Zip together the terms and the scores\n",
    "    tf_idf_tuples = list(zip(vectorizer.get_feature_names_out(), doc))\n",
    "    # Sort the terms by score\n",
    "    one_doc_as_df = pd.DataFrame.from_records(tf_idf_tuples, columns=['term', 'score']).sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "    # Add the date to the dataframe\n",
    "    one_doc_as_df['period'] = periods[counter]\n",
    "    # Append the dataframe to our list\n",
    "    tfidf_results.append(one_doc_as_df)\n",
    "# Concatenate all the dataframes together\n",
    "tfidf_df = pd.concat(tfidf_results)\n",
    "# Sort the dataframe by score\n",
    "tfidf_df = tfidf_df.sort_values(by=['score'], ascending=False)\n",
    "# Get the top thirty terms for each period\n",
    "top_terms = tfidf_df.groupby('period').apply(lambda x: x.sort_values('score', ascending=False).head(30)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_terms['period'] = top_terms['period'].astype(str)\n",
    "selection = alt.selection_point(fields=['term'], bind='legend')\n",
    "chart = alt.Chart(top_terms).mark_bar().encode(\n",
    "    y='score',\n",
    "    x=alt.X('period', sort=['early_internet', 'web_2.0', 'contemporary'], axis=alt.Axis(title='Period')),\n",
    "    color=alt.Color('term', legend=alt.Legend(title='Term', orient='right', symbolLimit=len(top_terms['term'].unique()), columns=5), scale=alt.Scale(scheme='tableau20')),\n",
    "    tooltip=['term', 'score', 'period'],\n",
    "    opacity=alt.condition(selection, alt.value(1), alt.value(0.2))\n",
    ").add_params(selection).properties(\n",
    "    title='Top 30 Terms by TF-IDF Score in Humanist Volumes by Period'\n",
    ")\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the code from our lesson in class. See if you can understand what it does. Then, try to modify it to see if you can get different results. Use the documentation to help you understand what each parameter does [https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html). In particular, try out the `n_components` and differing TF-IDF parameters. Also try using the pudding_data dataset to see if you can get different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text\n",
    "vectorizer = TfidfVectorizer(max_df=0.8)\n",
    "tfidf = vectorizer.fit_transform(humanist_vols['volume_text'])\n",
    "\n",
    "# Perform topic modeling\n",
    "lda = LatentDirichletAllocation(n_components=len(humanist_vols), max_iter=20, random_state=0)\n",
    "lda.fit(tfidf)\n",
    "\n",
    "# Get the top words for each topic\n",
    "top_words = vectorizer.get_feature_names_out()\n",
    "topic_words = {}\n",
    "for topic, comp in enumerate(lda.components_):\n",
    "    word_idx = np.argsort(comp)[::-1][:10]\n",
    "    topic_words[topic] = [top_words[i] for i in word_idx]\n",
    "\n",
    "# Print the top words for each topic\n",
    "for topic, words in topic_words.items():\n",
    "    print(f\"Topic #{topic}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out the code from our lesson in class. See if you can understand what it does. Then, try to modify it to see if you can get different results. Use the documentation to help you understand what each parameter does [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). In particular, try out the TFIDF parameters. Also try using the `gender_category` in the pudding_data dataset to see if you can get different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.8)\n",
    "# Fit the vectorizer to our documents\n",
    "transformed_documents = vectorizer.fit_transform(humanist_vols['volume_text'])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(transformed_documents, humanist_vols['period'], test_size=0.2, random_state=0)\n",
    "\n",
    "# Train a logistic regression classifier\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the time period of the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients for each term\n",
    "coefficients = clf.coef_\n",
    "# Get the terms\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "# Create a dataframe of the terms and coefficients\n",
    "terms_df = pd.DataFrame({'term': terms, 'contemporary': coefficients[0], 'early_internet': coefficients[1], 'web_2.0': coefficients[2]})\n",
    "# Get the top terms for each period\n",
    "top_terms = terms_df.melt(id_vars='term', var_name='period', value_name='coefficient').sort_values(by='coefficient', ascending=False).groupby('period').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize top terms\n",
    "top_terms['period'] = top_terms['period'].astype(str)\n",
    "selection = alt.selection_point(fields=['term'], bind='legend')\n",
    "\n",
    "# Define the sort order for the periods\n",
    "period_order = ['early_internet', 'web_2.0', 'contemporary']\n",
    "\n",
    "chart = alt.Chart(top_terms).mark_bar().encode(\n",
    "    x=alt.X('period', sort=['early_internet', 'web_2.0', 'contemporary'], axis=alt.Axis(title='Period')),\n",
    "    y=alt.Y('coefficient:Q'),  # Sort terms by score in descending order\n",
    "    color=alt.Color('term', legend=alt.Legend(title='Term', orient='right', symbolLimit=len(top_terms['term'].unique()), columns=5), scale=alt.Scale(scheme='tableau20')),\n",
    "    tooltip=['term', 'coefficient', 'period'],\n",
    "    opacity=alt.condition(selection, alt.value(1), alt.value(0.2))\n",
    ").add_params(selection).properties(\n",
    "    title='Top 10 Terms by Coefficient in Logistic Regression Model by Period'\n",
    ")\n",
    "chart"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-work-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
